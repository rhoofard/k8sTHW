
- hosts: 127.0.0.1
  tasks:
    - name: Create ca-csr.json
      #become: true
      copy:
        dest: ca-csr.json
        content: '{"CN":"Kubernetes","key":{"algo":"rsa","size":2048},"names":[{"C":"US","L":"Sacramento","O":"Kubernetes","OU":"CA","ST":"California"}]}'

    - name: Create ca-config.json
      #become: true
      copy:
        dest: ca-config.json
        content: '{"signing":{"default":{"expiry":"8760h"},"profiles":{"kubernetes":{"usages":["signing","key encipherment","server auth","client auth"],"expiry":"8760h"}}}}'

    - name: Create the ca.pem & ca-key.pem
      #become: true
      shell: |
        cfssl gencert -initca ca-csr.json | cfssljson -bare ca

    - name: Create admin-csr.json
      #become: true
      copy:
        dest: admin-csr.json
        content: '{"CN":"admin","key":{"algo":"rsa","size":2048},"names":[{"C":"US","L":"Sacramento","O":"system:masters","OU":"Kubernetes The Hard Way","ST":"California"}]}'

    - name: Create admin.pem & admin-key.pem
      #become: true
      shell: |
        cfssl gencert \
        -ca=ca.pem \
        -ca-key=ca-key.pem \
        -config=ca-config.json \
        -profile=kubernetes \
        admin-csr.json | cfssljson -bare admin

    - name: Create worker-*-csr.json
      #become: true
      copy:
        dest: worker-{{i}}-csr.json
        content: '{"CN":"system:node:worker-{{i}}","key":{"algo":"rsa","size":2048},"names":[{"C":"US","L":"Sacramento","O":"system:nodes","OU":"Kubernetes The Hard Way","ST":"California"}]}'
      loop: "{{ range (0, 2) }}"
      loop_control:
        index_var: i

    - name: Create worker-*.pem & worker-*-key.pem for kubelet
      # become: true
      shell: |
        EXTERNAL_IP=$(cat /tmp/worker_{{i}}_ip)
        INTERNAL_IP=10.240.0.2{{i}}
        cfssl gencert \
        -ca=ca.pem \
        -ca-key=ca-key.pem \
        -config=ca-config.json \
        -hostname=worker-{{i}},${EXTERNAL_IP},${INTERNAL_IP} \
        -profile=kubernetes \
        worker-{{i}}-csr.json | cfssljson -bare worker-{{i}}
      loop: "{{ range (0, 2) }}"
      loop_control:
        index_var: i
      
    - name: Copy kube-controller-manager.pem & kube-controller-manager-key.pem
      #become: true
      copy:
        dest: kube-controller-manager-csr.json
        content: '{"CN":"system:kube-controller-manager","key":{"algo":"rsa","size":2048},"names":[{"C":"US","L":"Sacramento","O":"system:kube-controller-manager","OU":"Kubernetes The Hard Way","ST":"California"}]}'

    - name: Generate certs
     #become: true
      shell: |
        cfssl gencert \
        -ca=ca.pem \
        -ca-key=ca-key.pem \
        -config=ca-config.json \
        -profile=kubernetes \
        kube-controller-manager-csr.json | cfssljson -bare kube-controller-manager
      
    - name: Copy kube-proxy-key.pem & kube-proxy.pem
      #become: true
      copy:
        dest: kube-proxy-csr.json
        content: '{"CN":"system:kube-proxy","key":{"algo":"rsa","size":2048},"names":[{"C":"US","L":"Sacramento","O":"system:node-proxier","OU":"Kubernetes The Hard Way","ST":"California"}]}'

    - name: Generate certs
      #become: true
      shell: |
        cfssl gencert \
        -ca=ca.pem \
        -ca-key=ca-key.pem \
        -config=ca-config.json \
        -profile=kubernetes \
        kube-proxy-csr.json | cfssljson -bare kube-proxy
    
    - name: Copy kube-scheduler-key.pem & kube-scheduler.pem
      #become: true
      copy:
        dest: kube-scheduler-csr.json
        content: '{"CN":"system:kube-scheduler","key":{"algo":"rsa","size":2048},"names":[{"C":"US","L":"Sacramento","O":"system:kube-scheduler","OU":"Kubernetes The Hard Way","ST":"California"}]}'

    - name: Generate certs
      #become: true
      shell: |
        cfssl gencert \
        -ca=ca.pem \
        -ca-key=ca-key.pem \
        -config=ca-config.json \
        -profile=kubernetes \
        kube-scheduler-csr.json | cfssljson -bare kube-scheduler
      
    - name: Generate kubernetes-csr.json
        #become: true
      copy:
        dest: kubernetes-csr.json
        content: '{"CN":"kubernetes","key":{"algo":"rsa","size":2048},"names":[{"C":"US","L":"Sacramento","O":"Kubernetes","OU":"Kubernetes The Hard Way","ST":"California"}]}'

    - name: Copy kubernetes-key.pem & kubernetes.pem
        #become: true
      shell: |
          KUBERNETES_PUBLIC_ADDRESS=$(cat /tmp/lb-address)
          KUBERNETES_HOSTNAMES="kubernetes,kubernetes.default,kubernetes.default.svc,kubernetes.default.svc.cluster,kubernetes.svc.cluster.local"
          cfssl gencert \
          -ca=ca.pem \
          -ca-key=ca-key.pem \
          -config=ca-config.json \
          -hostname=10.32.0.1,10.240.0.10,10.240.0.11,10.240.0.12,${KUBERNETES_PUBLIC_ADDRESS},127.0.0.1,${KUBERNETES_HOSTNAMES} \
          -profile=kubernetes \
          kubernetes-csr.json | cfssljson -bare kubernetes

    - name: Copy service-account-key.pem & service-account.pem
      #become: true
      copy:
        dest: service-account-csr.json
        content: '{"CN":"service-accounts","key":{"algo":"rsa","size":2048},"names":[{"C":"US","L":"Sacramento","O":"Kubernetes","OU":"Kubernetes The Hard Way","ST":"California"}]}'

    - name: Generate certs
     # become: true
      shell: |
        cfssl gencert \
        -ca=ca.pem \
        -ca-key=ca-key.pem \
        -config=ca-config.json \
        -profile=kubernetes \
        service-account-csr.json | cfssljson -bare service-account


- hosts: worker_0
  tasks:
    - name: Copy ca.pem worker-0-key.pem worker-0.pem to worker_0
      copy:
        src: "{{ item }}"
        dest: "."
      with_items:
        - ca.pem
        - worker-0-key.pem
        - worker-0.pem   

- hosts: worker_1
  tasks:
    - name: Copy ca.pem worker-1-key.pem worker-1.pem to worker_1
      copy:
        src: "{{ item }}"
        dest: "."
      with_items:
        - ca.pem
        - worker-1-key.pem
        - worker-1.pem   

- hosts: controllers
  tasks:
    - name: Copy certs to controllers
      copy:
        src: "{{ item }}"
        dest: "."
      with_items:
        - ca.pem
        - ca-key.pem
        - kubernetes-key.pem 
        - kubernetes.pem
        - service-account-key.pem
        - service-account.pem    
  
- hosts: 127.0.0.1
  tasks:
    - name: Generate kubeconfig for each worker node
      shell: |
        KUBERNETES_PUBLIC_ADDRESS=$(cat /tmp/lb_ip)
        kubectl config set-cluster kubernetes-the-hard-way \
        --certificate-authority=ca.pem \
        --embed-certs=true \
        --server=https://${KUBERNETES_PUBLIC_ADDRESS}:6443 \
        --kubeconfig=worker-{{i}}.kubeconfig
        kubectl config set-credentials system:node:worker-{{i}} \
        --client-certificate=worker-{{i}}.pem \
        --client-key=worker-{{i}}-key.pem \
        --embed-certs=true \
        --kubeconfig=worker-{{i}}.kubeconfig
        kubectl config set-context default \
        --cluster=kubernetes-the-hard-way \
        --user=system:node:worker-{{i}} \
        --kubeconfig=worker-{{i}}.kubeconfig
        kubectl config use-context default --kubeconfig=worker-{{i}}.kubeconfig
      loop: "{{ range (0, 2) }}"
      loop_control:
        index_var: i

    - name: Generate kube-proxy
      shell: |
        KUBERNETES_PUBLIC_ADDRESS=$(cat /tmp/lb_ip)
        kubectl config set-cluster kubernetes-the-hard-way \
        --certificate-authority=ca.pem \
        --embed-certs=true \
        --server=https://${KUBERNETES_PUBLIC_ADDRESS}:6443 \
        --kubeconfig=kube-proxy.kubeconfig
        kubectl config set-credentials system:kube-proxy \
          --client-certificate=kube-proxy.pem \
          --client-key=kube-proxy-key.pem \
          --embed-certs=true \
          --kubeconfig=kube-proxy.kubeconfig
        kubectl config set-context default \
          --cluster=kubernetes-the-hard-way \
          --user=system:kube-proxy \
          --kubeconfig=kube-proxy.kubeconfig
        kubectl config use-context default --kubeconfig=kube-proxy.kubeconfig

    - name: Generate kube-controller-manager
      shell: |
        KUBERNETES_PUBLIC_ADDRESS=$(cat /tmp/lb_ip)
        kubectl config set-cluster kubernetes-the-hard-way \
        --certificate-authority=ca.pem \
        --embed-certs=true \
        --server=https://127.0.0.1:6443 \
        --kubeconfig=kube-controller-manager.kubeconfig
        kubectl config set-credentials system:kube-controller-manager \
          --client-certificate=kube-controller-manager.pem \
          --client-key=kube-controller-manager-key.pem \
          --embed-certs=true \
          --kubeconfig=kube-controller-manager.kubeconfig
        kubectl config set-context default \
          --cluster=kubernetes-the-hard-way \
          --user=system:kube-controller-manager \
          --kubeconfig=kube-controller-manager.kubeconfig
        kubectl config use-context default --kubeconfig=kube-controller-manager.kubeconfig

    - name: Generate kube-scheduler
      shell: |
        KUBERNETES_PUBLIC_ADDRESS=$(cat /tmp/lb_ip)
        kubectl config set-cluster kubernetes-the-hard-way \
        --certificate-authority=ca.pem \
        --embed-certs=true \
        --server=https://127.0.0.1:6443 \
        --kubeconfig=kube-scheduler.kubeconfig
        kubectl config set-credentials system:kube-scheduler \
          --client-certificate=kube-scheduler.pem \
          --client-key=kube-scheduler-key.pem \
          --embed-certs=true \
          --kubeconfig=kube-scheduler.kubeconfig
        kubectl config set-context default \
          --cluster=kubernetes-the-hard-way \
          --user=system:kube-scheduler \
          --kubeconfig=kube-scheduler.kubeconfig
        kubectl config use-context default --kubeconfig=kube-scheduler.kubeconfig

    - name: Generate admin
      shell: |
        KUBERNETES_PUBLIC_ADDRESS=$(cat /tmp/lb_ip)
        kubectl config set-cluster kubernetes-the-hard-way \
        --certificate-authority=ca.pem \
        --embed-certs=true \
        --server=https://127.0.0.1:6443 \
        --kubeconfig=admin.kubeconfig
        kubectl config set-credentials admin \
          --client-certificate=admin.pem \
          --client-key=admin-key.pem \
          --embed-certs=true \
          --kubeconfig=admin.kubeconfig
        kubectl config set-context default \
          --cluster=kubernetes-the-hard-way \
          --user=admin \
          --kubeconfig=admin.kubeconfig
        kubectl config use-context default --kubeconfig=admin.kubeconfig

- hosts: worker_0
  tasks:
    - name: kube stuff to worker 0
      copy:
        src: "{{ item }}"
        dest: "."
      with_items:
        - worker-0.kubeconfig
        - kube-proxy.kubeconfig 
      

- hosts: worker_1
  tasks:
    - name: kube stuff to worker 1
      copy:
        src: "{{ item }}"
        dest: "."
      with_items:
        - worker-1.kubeconfig
        - kube-proxy.kubeconfig   
      

- hosts: controllers
  tasks:
    - name: Copy kube stuff to controllers
      copy:
        src: "{{ item }}"
        dest: "."
      with_items:
        - admin.kubeconfig
        - kube-controller-manager.kubeconfig 
        - kube-scheduler.kubeconfig     

- hosts: 127.0.0.1
  tasks:
    - name: Create encryption-config.yaml
      shell: |
        ENCRYPTION_KEY=$(head -c 32 /dev/urandom | base64)
        cat > encryption-config.yaml <<EOF
        kind: EncryptionConfig
        apiVersion: v1
        resources:
          - resources:
              - secrets
            providers:
              - aescbc:
                  keys:
                    - name: key1
                      secret: ${ENCRYPTION_KEY}
              - identity: {}
        EOF
- hosts: controllers
  tasks:
    - name: Copy encryption-config.yaml to controllers
      copy:
        src: encryption-config.yaml
        dest: "."
    
    - name: download etcd binaries
      get_url:
        url: "{{ item }}"
        dest: /tmp
      with_items:
        - https://github.com/etcd-io/etcd/releases/download/v3.3.20/etcd-v3.3.20-linux-amd64.tar.gz

    - name: untar
      shell: |
        tar -xvf /tmp/etcd-v3.3.20-linux-amd64.tar.gz

    - name: move etcd
      shell: |
        sudo mv etcd-v3.3.20-linux-amd64/etcd* /usr/local/bin/
        sudo mkdir -p /etc/etcd /var/lib/etcd
        sudo cp ca.pem kubernetes-key.pem kubernetes.pem /etc/etcd/

    - name: configure etcd server
      shell: |
        INTERNAL_IP=$(ip addr show eth0 | grep -oP '(?<=inet\s)\d+(\.\d+){3}')
        ETCD_NAME=$(hostname -s)

        cat > etcd.service <<EOF
        [Unit]
        Description=etcd
        Documentation=https://github.com/coreos

        [Service]
        Type=notify
        ExecStart=/usr/local/bin/etcd \\
          --name ${ETCD_NAME} \\
          --cert-file=/etc/etcd/kubernetes.pem \\
          --key-file=/etc/etcd/kubernetes-key.pem \\
          --peer-cert-file=/etc/etcd/kubernetes.pem \\
          --peer-key-file=/etc/etcd/kubernetes-key.pem \\
          --trusted-ca-file=/etc/etcd/ca.pem \\
          --peer-trusted-ca-file=/etc/etcd/ca.pem \\
          --peer-client-cert-auth \\
          --client-cert-auth \\
          --initial-advertise-peer-urls https://${INTERNAL_IP}:2380 \\
          --listen-peer-urls https://${INTERNAL_IP}:2380 \\
          --listen-client-urls https://${INTERNAL_IP}:2379,https://127.0.0.1:2379 \\
          --advertise-client-urls https://${INTERNAL_IP}:2379 \\
          --initial-cluster-token etcd-cluster-0 \\
          --initial-cluster controller-0=https://10.240.0.10:2380,controller-1=https://10.240.0.11:2380,controller-2=https://10.240.0.12:2380 \\
          --initial-cluster-state new \\
          --data-dir=/var/lib/etcd
        Restart=on-failure
        RestartSec=5

        [Install]
        WantedBy=multi-user.target
        EOF
    
    - pause:
        seconds: 5

    - name: move newly created file
      shell: |
        sudo mv etcd.service /etc/systemd/system/

    - name: starting server
      shell: |
        sudo systemctl daemon-reload
        sudo systemctl enable etcd
        sudo systemctl start etcd

    - name: Create the Kubernetes configuration directory
      become: true
      shell: |
        sudo mkdir -p /etc/kubernetes/config

    - name: Download kube-apiserver, kube-controller-manager, kube-scheduler and kubectl
      get_url:
        url: "{{ item }}"
        dest: /tmp
      with_items:
        - https://storage.googleapis.com/kubernetes-release/release/v1.17.3/bin/linux/amd64/kube-apiserver
        - https://storage.googleapis.com/kubernetes-release/release/v1.17.3/bin/linux/amd64/kube-controller-manager
        - https://storage.googleapis.com/kubernetes-release/release/v1.17.3/bin/linux/amd64/kube-scheduler
        - https://storage.googleapis.com/kubernetes-release/release/v1.17.3/bin/linux/amd64/kubectl


    - name: Changing perm of binaries in /tmp, adding "+x"
      shell: |
        chmod +x /tmp/kube-apiserver /tmp/kube-controller-manager /tmp/kube-scheduler /tmp/kubectl
        sudo mv /tmp/kube-apiserver /tmp/kube-controller-manager /tmp/kube-scheduler /tmp/kubectl /usr/local/bin/

    - name: Configure the Kubernetes API Server
      shell: |
        sudo mkdir -p /var/lib/kubernetes/
        sudo cp ca.pem ca-key.pem kubernetes-key.pem kubernetes.pem \
        service-account-key.pem service-account.pem \
        encryption-config.yaml /var/lib/kubernetes/  

    - name: Create the kube-apiserver.service systemd unit file
      shell: |
        INTERNAL_IP=$(ip addr show eth0 | grep -oP '(?<=inet\s)\d+(\.\d+){3}')
        cat <<EOF | sudo tee /etc/systemd/system/kube-apiserver.service
        [Unit]
        Description=Kubernetes API Server
        Documentation=https://github.com/kubernetes/kubernetes

        [Service]
        ExecStart=/usr/local/bin/kube-apiserver \\
          --advertise-address=${INTERNAL_IP} \\
          --allow-privileged=true \\
          --apiserver-count=2 \\
          --audit-log-maxage=30 \\
          --audit-log-maxbackup=3 \\
          --audit-log-maxsize=100 \\
          --audit-log-path=/var/log/audit.log \\
          --authorization-mode=Node,RBAC \\
          --bind-address=0.0.0.0 \\
          --client-ca-file=/var/lib/kubernetes/ca.pem \\
          --enable-admission-plugins=NamespaceLifecycle,LimitRanger,ServiceAccount,TaintNodesByCondition,Priority,DefaultTolerationSeconds,DefaultStorageClass,PersistentVolumeClaimResize,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota \\
          --enable-swagger-ui=true \\
          --etcd-cafile=/var/lib/kubernetes/ca.pem \\
          --etcd-certfile=/var/lib/kubernetes/kubernetes.pem \\
          --etcd-keyfile=/var/lib/kubernetes/kubernetes-key.pem \\
          --etcd-servers=https://10.240.0.10:2379,https://10.240.0.11:2379,https://10.240.0.12:2379 \\
          --event-ttl=1h \\
          --experimental-encryption-provider-config=/var/lib/kubernetes/encryption-config.yaml \\
          --kubelet-certificate-authority=/var/lib/kubernetes/ca.pem \\
          --kubelet-client-certificate=/var/lib/kubernetes/kubernetes.pem \\
          --kubelet-client-key=/var/lib/kubernetes/kubernetes-key.pem \\
          --kubelet-https=true \\
          --runtime-config=api/all=true \\
          --service-account-key-file=/var/lib/kubernetes/service-account.pem \\
          --service-cluster-ip-range=10.32.0.0/24 \\
          --service-node-port-range=30000-32767 \\
          --tls-cert-file=/var/lib/kubernetes/kubernetes.pem \\
          --tls-private-key-file=/var/lib/kubernetes/kubernetes-key.pem \\
          --v=2
        Restart=on-failure
        RestartSec=5

        [Install]
        WantedBy=multi-user.target   
        EOF

    - name: Create the kube-controller-manager.service systemd unit file
      shell: |         
        sudo mv kube-controller-manager.kubeconfig /var/lib/kubernetes/  

        cat <<EOF | sudo tee /etc/systemd/system/kube-controller-manager.service
        [Unit]
        Description=Kubernetes Controller Manager
        Documentation=https://github.com/kubernetes/kubernetes

        [Service]
        ExecStart=/usr/local/bin/kube-controller-manager \\
          --address=0.0.0.0 \\
          --allocate-node-cidrs=true \\
          --cluster-cidr=10.200.0.0/16 \\
          --cluster-name=kubernetes \\
          --cluster-signing-cert-file=/var/lib/kubernetes/ca.pem \\
          --cluster-signing-key-file=/var/lib/kubernetes/ca-key.pem \\
          --kubeconfig=/var/lib/kubernetes/kube-controller-manager.kubeconfig \\
          --leader-elect=true \\
          --root-ca-file=/var/lib/kubernetes/ca.pem \\
          --service-account-private-key-file=/var/lib/kubernetes/service-account-key.pem \\
          --service-cluster-ip-range=10.32.0.0/24 \\
          --use-service-account-credentials=true \\
          --v=2
        Restart=on-failure
        RestartSec=5

        [Install]
        WantedBy=multi-user.target
        EOF

    - name: Move the kube-scheduler kubeconfig into place and create kube-scheduler.yaml
      shell: |
        sudo mv kube-scheduler.kubeconfig /var/lib/kubernetes/
        cat <<EOF | sudo tee /etc/kubernetes/config/kube-scheduler.yaml
        apiVersion: kubescheduler.config.k8s.io/v1alpha1
        kind: KubeSchedulerConfiguration
        clientConnection:
          kubeconfig: "/var/lib/kubernetes/kube-scheduler.kubeconfig"
        leaderElection:
          leaderElect: true


    - name: Create the kube-scheduler.service systemd unit file
      shell: |
        cat <<EOF | sudo tee /etc/systemd/system/kube-scheduler.service
        [Unit]
        Description=Kubernetes Scheduler
        Documentation=https://github.com/kubernetes/kubernetes
        [Service]
        ExecStart=/usr/local/bin/kube-scheduler \\
          --config=/etc/kubernetes/config/kube-scheduler.yaml \\
          --v=2
        Restart=on-failure
        RestartSec=5

        [Install]
        WantedBy=multi-user.target
        

    - name: Start/Enable the Controller Services
      shell: |
        sudo systemctl daemon-reload
        sudo systemctl enable kube-apiserver kube-controller-manager kube-scheduler
        sudo systemctl start kube-apiserver kube-controller-manager kube-scheduler               
  
    - name: RBAC cluster role
      shell: |
        cat <<EOF | kubectl apply -f -
        apiVersion: rbac.authorization.k8s.io/v1
        kind: ClusterRole
        metadata:
          annotations:
            rbac.authorization.kubernetes.io/autoupdate: "true"
          labels:
            kubernetes.io/bootstrapping: rbac-defaults
          name: system:kube-apiserver-to-kubelet
        rules:
          - apiGroups:
              - ""
            resources:
              - nodes/proxy
              - nodes/stats
              - nodes/log
              - nodes/spec
              - nodes/metrics
            verbs:
              - "*"
        EOF

    - name: RBAC cluster role binding
      shell: |
        cat <<EOF | kubectl apply -f -
        apiVersion: rbac.authorization.k8s.io/v1
        kind: ClusterRoleBinding
        metadata:
          name: system:kube-apiserver
          namespace: ""
        roleRef:
          apiGroup: rbac.authorization.k8s.io
          kind: ClusterRole
          name: system:kube-apiserver-to-kubelet
        subjects:
          - apiGroup: rbac.authorization.k8s.io
            kind: User
            name: kubernetes
        EOF

- hosts: workers
  tasks:
    - name: updating packages
      shell: |
        sudo apt-get update
        sudo apt-get -y install socat conntrack ipset
    
    - name: downloading binaries
      get_url:
        url: "{{ item }}"
        dest: "."
      with_items:
        - https://github.com/kubernetes-sigs/cri-tools/releases/download/v1.17.0/crictl-v1.17.0-linux-amd64.tar.gz 
        - https://storage.googleapis.com/gvisor/releases/nightly/latest/runsc 
        - https://github.com/opencontainers/runc/releases/download/v1.0.0-rc10/runc.amd64
        - https://github.com/containernetworking/plugins/releases/download/v0.8.5/cni-plugins-linux-amd64-v0.8.5.tgz 
        - https://github.com/containerd/containerd/releases/download/v1.3.2/containerd-1.3.2.linux-amd64.tar.gz 
        - https://storage.googleapis.com/kubernetes-release/release/v1.17.3/bin/linux/amd64/kubectl 
        - https://storage.googleapis.com/kubernetes-release/release/v1.17.3/bin/linux/amd64/kube-proxy 
        - https://storage.googleapis.com/kubernetes-release/release/v1.17.3/bin/linux/amd64/kubelet
    
    - name: make directories
      shell: |
        sudo mkdir -p \
        /etc/cni/net.d \
        /opt/cni/bin \
        /var/lib/kubelet \
        /var/lib/kube-proxy \
        /var/lib/kubernetes \
        /var/run/kubernetes
    
    - name: install worker binaries
      shell: |
        sudo mv runc.amd64 runc
        chmod +x kubectl kube-proxy kubelet runc runsc
        sudo mv kubectl kube-proxy kubelet runc runsc /usr/local/bin/
        sudo tar -xvf crictl-v1.17.0-linux-amd64.tar.gz -C /usr/local/bin/
        sudo tar -xvf cni-plugins-linux-amd64-v0.8.5.tgz -C /opt/cni/bin/
        sudo tar -xvf containerd-1.3.2.linux-amd64.tar.gz -C /

    - name: configure cni network
      shell: |
        POD_CIDR="$(echo $(curl --silent -H Metadata:true "http://169.254.169.254/metadata/instance/compute/tags?api-version=2017-08-01&format=text") | cut -d : -f2)"
        cat <<EOF | sudo tee /etc/cni/net.d/10-bridge.conf
        {
            "cniVersion": "0.3.0",
            "name": "bridge",
            "type": "bridge",
            "bridge": "cnio0",
            "isGateway": true,
            "ipMasq": true,
            "ipam": {
                "type": "host-local",
                "ranges": [
                  [{"subnet": "${POD_CIDR}"}]
                ],
                "routes": [{"dst": "0.0.0.0/0"}]
            }
        }
        EOF
    
    - name: create loopback config
      shell: |
        cat <<EOF | sudo tee /etc/cni/net.d/99-loopback.conf
        {
            "cniVersion": "0.3.0",
            "name": "lo",
            "type": "loopback"
        }
        EOF
    
    - name: make containerd file
      shell: |
        sudo mkdir -p /etc/containerd/

        cat << EOF | sudo tee /etc/containerd/config.toml
        [plugins]
          [plugins.cri.containerd]
            snapshotter = "overlayfs"
            [plugins.cri.containerd.default_runtime]
              runtime_type = "io.containerd.runtime.v1.linux"
              runtime_engine = "/usr/local/bin/runc"
              runtime_root = ""
            [plugins.cri.containerd.untrusted_workload_runtime]
              runtime_type = "io.containerd.runtime.v1.linux"
              runtime_engine = "/usr/local/bin/runsc"
              runtime_root = "/run/containerd/runsc"
            [plugins.cri.containerd.gvisor]
              runtime_type = "io.containerd.runtime.v1.linux"
              runtime_engine = "/usr/local/bin/runsc"
              runtime_root = "/run/containerd/runsc"
        EOF

    - name: create containerd.service
      shell: |
        cat <<EOF | sudo tee /etc/systemd/system/containerd.service
        [Unit]
        Description=containerd container runtime
        Documentation=https://containerd.io
        After=network.target

        [Service]
        ExecStartPre=/sbin/modprobe overlay
        ExecStart=/bin/containerd

        Delegate=yes
        KillMode=process
        # Having non-zero Limit*s causes performance problems due to accounting overhead
        # in the kernel. We recommend using cgroups to do container-local accounting.
        LimitNPROC=infinity
        LimitCORE=infinity
        LimitNOFILE=infinity
        # Comment TasksMax if your systemd version does not supports it.
        # Only systemd 226 and above support this version.
        TasksMax=infinity

        [Install]
        WantedBy=multi-user.target
        EOF

    - name: configure kubelet
      shell: |
        sudo mv ${HOSTNAME}-key.pem ${HOSTNAME}.pem /var/lib/kubelet/
        sudo mv ${HOSTNAME}.kubeconfig /var/lib/kubelet/kubeconfig
        sudo mv ca.pem /var/lib/kubernetes/

    - name: kubelet config
      shell: |
        cat <<EOF | sudo tee /var/lib/kubelet/kubelet-config.yaml
        kind: KubeletConfiguration
        apiVersion: kubelet.config.k8s.io/v1beta1
        authentication:
          anonymous:
            enabled: false
          webhook:
            enabled: true
          x509:
            clientCAFile: "/var/lib/kubernetes/ca.pem"
        authorization:
          mode: Webhook
        clusterDomain: "cluster.local"
        clusterDNS:
          - "10.32.0.10"
        podCIDR: "${POD_CIDR}"
        resolvConf: "/run/systemd/resolve/resolv.conf"
        runtimeRequestTimeout: "15m"
        tlsCertFile: "/var/lib/kubelet/${HOSTNAME}.pem"
        tlsPrivateKeyFile: "/var/lib/kubelet/${HOSTNAME}-key.pem"
        EOF

    - name: kubelet.service
      shell: |
        cat <<EOF | sudo tee /etc/systemd/system/kubelet.service
        [Unit]
        Description=Kubernetes Kubelet
        Documentation=https://github.com/kubernetes/kubernetes
        After=containerd.service
        Requires=containerd.service

        [Service]
        ExecStart=/usr/local/bin/kubelet \\
          --config=/var/lib/kubelet/kubelet-config.yaml \\
          --container-runtime=remote \\
          --container-runtime-endpoint=unix:///var/run/containerd/containerd.sock \\
          --image-pull-progress-deadline=2m \\
          --kubeconfig=/var/lib/kubelet/kubeconfig \\
          --network-plugin=cni \\
          --register-node=true \\
          --v=2
        Restart=on-failure
        RestartSec=5

        [Install]
        WantedBy=multi-user.target
        EOF

    - name: kube proxy config
      shell: |
        sudo mv kube-proxy.kubeconfig /var/lib/kube-proxy/kubeconfig

        cat <<EOF | sudo tee /var/lib/kube-proxy/kube-proxy-config.yaml
        kind: KubeProxyConfiguration
        apiVersion: kubeproxy.config.k8s.io/v1alpha1
        clientConnection:
          kubeconfig: "/var/lib/kube-proxy/kubeconfig"
        mode: "iptables"
        clusterCIDR: "10.200.0.0/16"
        EOF

    - name: kube proxy service systemd
      shell: |
        cat <<EOF | sudo tee /etc/systemd/system/kube-proxy.service
        [Unit]
        Description=Kubernetes Kube Proxy
        Documentation=https://github.com/kubernetes/kubernetes

        [Service]
        ExecStart=/usr/local/bin/kube-proxy \\
          --config=/var/lib/kube-proxy/kube-proxy-config.yaml
        Restart=on-failure
        RestartSec=5

        [Install]
        WantedBy=multi-user.target
        EOF

    - name: start workers service
      shell: |
        sudo systemctl daemon-reload
        sudo systemctl enable containerd kubelet kube-proxy
        sudo systemctl start containerd kubelet kube-proxy